{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE4375-2022: Fifth Lab Session: One-Dimensional Galerkin Finite Element Method using Distributed Memory Parallel Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solves the Poisson equation $- \\frac{d^2 \\, u(x)}{dx^2} = f(x)$ on the unit bar domain $x \\in \\Omega=(0,1)$ supplied with various boundary conditions and various source terms. The Galerkin finite element method is employed. Here we target a parallel computing (using distributed memory) imnplementation. \n",
    "\n",
    "This problem can be solved using [GridapDistributed](https://gridap.github.io/GridapDistributed.jl/dev/) as students in the bachelor minor Computational Science and Engineering have convincingly shown. It remains valuable to dig for the details. \n",
    "\n",
    "General info on [parallel computing in Julia](https://juliaparallel.org/resources/) and [MPI.jl](https://github.com/JuliaParallel/MPI.jl). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "using LaTeXStrings\n",
    "@everywhere using SparseArrays\n",
    "using BenchmarkTools\n",
    "using Distributed\n",
    "@everywhere using SharedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Preprocessing \n",
    "- <b> shared memory approach </b>: monolytic in memory approach: assuming global mesh, matrix and rhs vector to be available on all processors: assembly process similar to all processors; \n",
    "- <b> distributed memory approach </b>: distribute memory approach: distribute memory approach: distributed assembly and solve; include figure here; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Shared Memory Assembly of Matrix and Right-Hand Side Vector \n",
    "- <b> decompose mesh elements over the processors </b>: observe mesh to be decomposed according to elements and <b>not</b> according to nodes; \n",
    "- <b> use shared memory for-loop over elements </b>: the for-loop over elements can run in parallel in shared memory using either the macro @distributed or pmap function from the  [distributed computing section](https://docs.julialang.org/en/v1/stdlib/Distributed/) of the standard library; \n",
    "- <b> used SharedVector and SharedMatrix to store answers</b>: using SharedVector and SharedMatrix using [shared arrays](https://docs.julialang.org/en/v1/stdlib/SharedArrays/); each processor fills its part of the matrix; \n",
    "- not sure whether [Distributed Arrays](https://juliaparallel.org/DistributedArrays.jl/stable/) offers any advantages; \n",
    "- valuable example on the use of SharedArray is [sharedarrays-and-data-movement-across-processes-in-julia](https://stackoverflow.com/questions/53753789/sharedarrays-and-data-movement-across-processes-in-julia)\n",
    "- a schematic representation of the shared memory implementation would be valuable to have; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation on the following would be valuable to add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small example on how to initialize a shared vector and matrix. \n",
    "# observe the matrix to be dense.  \n",
    "f = SharedVector{Float64}(5)\n",
    "A = SharedArray{Float64}((5,5))\n",
    "procs(A) # what does this do? \n",
    "localindices(A) # what does this do? \n",
    "indexpids(A) # what does this do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory Implementation of 1D Galerkin FEM: Option-1 \n",
    "The implementation of Option-1 employs SharedArrays to store the coefficient matrix. This implementation requires storing the coefficient matrix as a <b>dense</b> matrix. It is thus inadequate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x00000002898f7540"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start REPL using julia -p 5 \n",
    "addprocs(4)\n",
    "no_procs = nprocs()\n",
    "\n",
    "#..construct the mesh: see before \n",
    "@everywhere N = 100; \n",
    "@everywhere h = 1/N; \n",
    "@everywhere x = Vector(0:h:1); \n",
    "\n",
    "#..Mesh with points and edges \n",
    "#..point holds the coordinates of the left and right node of the element\n",
    "#..edges holds the global indices of the left and right node of the element\n",
    "@everywhere points = collect( [x[i], x[i+1]] for i in 1:length(x)-1) \n",
    "@everywhere edges = collect( [i, i+1] for i in 1:length(x)-1); \n",
    "\n",
    "#..Set the source function \n",
    "@everywhere fsource(x) = x*(x-1); \n",
    "\n",
    "#..Initialize global matrix and right-hand side value \n",
    "A = SharedArray{Float64}((length(x), length(x))); # type annotation required here \n",
    "f = SharedVector{Float64}(length(x)); # type annotation required here \n",
    "\n",
    "#..Perform loop over elements and assemble global matrix and vector \n",
    "@distributed for i=1:length(edges) \n",
    "  xl, xr = points[i,:][1]\n",
    "  floc = (xr-xl) * [fsource(xl); fsource(xr)];\n",
    "  Aloc = (1/(xr-xl))*[1 -1; -1 1]; \n",
    "  f[edges[i]] += floc\n",
    "  A[edges[i], edges[i]] += Aloc\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory Implementation of 1D Galerkin FEM: Option-2 \n",
    "The implementation of Option-2 employs sparse matrix to store the coefficient matrix. All threads see the complete matrix. This implementation does not pre-allocate the memory of the sparse matrix and is thus inadequate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start REPL using julia -p 5 \n",
    "addprocs(4)\n",
    "no_procs = nprocs()\n",
    "\n",
    "#..construct the mesh: see before \n",
    "@everywhere N = 100; \n",
    "@everywhere h = 1/N; \n",
    "@everywhere x = Vector(0:h:1); \n",
    "\n",
    "#..Mesh with points and edges \n",
    "#..point holds the coordinates of the left and right node of the element\n",
    "#..edges holds the global indices of the left and right node of the element\n",
    "@everywhere points = collect( [x[i], x[i+1]] for i in 1:length(x)-1) \n",
    "@everywhere edges = collect( [i, i+1] for i in 1:length(x)-1); \n",
    "\n",
    "#..Set the source function \n",
    "@everywhere fsource(x) = x*(x-1); \n",
    "\n",
    "#..Initialize global matrix and right-hand side value \n",
    "@everywhere A = spzeros(Float64,length(x),length(x))\n",
    "@everywhere f = zeros(Float64,length(x), 1)\n",
    "\n",
    "#..Perform loop over elements and assemble global matrix and vector \n",
    "@distributed for i=1:length(edges) \n",
    "  xl, xr = points[i,:][1]\n",
    "  floc = (xr-xl) * [fsource(xl); fsource(xr)];\n",
    "  Aloc = (1/(xr-xl))*[1 -1; -1 1]; \n",
    "  f[edges[i]] += floc\n",
    "  A[edges[i], edges[i]] += Aloc\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Memory Implementation of 1D Galerkin FEM: Option-3 (in progress) \n",
    "The implementation of Option-3 employs the sparse command sequentially on three lists generated in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struct to hold single element\n",
    "struct Element\n",
    "  p1::Float64\n",
    "  p2::Float64\n",
    "  e1::Int64\n",
    "  e2::Int64\n",
    "end \n",
    "\n",
    "function fem_1d_sparse(N)\n",
    "    \n",
    "    if (Bool(0)) print(\" [fem_1d]:: input N = \", N, \"\\n\") end \n",
    "    \n",
    "    #..Generate the mesh \n",
    "    Np1 = N+1; h = 1/N\n",
    "    x = Vector(0:h:1) \n",
    "    mesh = StructArray{Element}((x[1:end-1], x[2:end], Vector(1:N), Vector(2:N+1)))\n",
    "\n",
    "    if (Bool(0)) print(\" [fem_1d]:: mesh = \", mesh, \"\\n\") end\n",
    "\n",
    "    #..Set the source function \n",
    "    fsource(x) = x*(x-1)\n",
    "\n",
    "    #..Initialize global matrix and right-hand side value \n",
    "    f = zeros(Float64,Np1,1)\n",
    "    I = zeros(Int64,4*N)\n",
    "    J = zeros(Int64,4*N)\n",
    "    Avalues = zeros(Float64,4*N)\n",
    "    floc = zeros(Float64,2, 1)\n",
    "    Aloc = zeros(Float64,2,2)\n",
    "\n",
    "    #..Perform loop over elements and assemble global matrix and vector \n",
    "    @inbounds for i=1:N \n",
    "        \n",
    "      xl = mesh[i].p1\n",
    "      xr = mesh[i].p2\n",
    "      j  = mesh[i].e1\n",
    "      k  = mesh[i].e2\n",
    "        \n",
    "      floc = (xr-xl) * [fsource(xl), fsource(xr)];\n",
    "      Aloc = (1/(xr-xl))*[1, -1, -1, 1]; \n",
    "        \n",
    "      f[[j,k]] += floc \n",
    "      I[4*(i-1)+1:4*i] = [j, k, j, k]\n",
    "      J[4*(i-1)+1:4*i] = [j, j, k, k]\n",
    "      Avalues[4*(i-1)+1:4*i] = Aloc \n",
    "        \n",
    "    end\n",
    "\n",
    "    A = sparse(I,J,Avalues)\n",
    "\n",
    "    #..handle the boundary conditions in the matrix and right-hand side vector \n",
    "    A[1,1] = 1;     A[1,2] = 0;        f[1]   = 0\n",
    "    A[end,end-1]=0; A[end,end] = 1;    f[end] = 0\n",
    "\n",
    "    #..solve the linear system\n",
    "    u = A \\ f  \n",
    "    \n",
    "    return x, u \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions \n",
    "1. does increasing number of processors decrease overall CPU time? \n",
    "2. can @distributed for be replace by pmap? See [this link](https://jishnub.github.io/ParallelUtilities.jl/dev/examples/sharedarrays/). Does it offer advantages? \n",
    "3. do sparse distributed arrays exist? How to perform memory allocations of these arrays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Distributed Memory Assembly of Matrix and Right-Hand Side Vector \n",
    "- using [PartionedMatrices](https://github.com/fverdugo/PartitionedArrays.jl); need to study the example described at [example](https://www.francescverdugo.com/PartitionedArrays.jl/dev/usage/).  \n",
    "\n",
    "<div>\n",
    "<img src=\"./figures/dag-fem-1d.jpg\" width=400 /> \n",
    "<center> Figure 1: Directed acyclic graph representation of decomposed mesh of 4 elements on the interval. A finite element discretization is assumed. </center>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Shared Memory Linear System Solve \n",
    "- <b> first approach </b>: use backslash (seems to works, not clear what is does internally); \n",
    "- <b> second approach </b>: use packages such as [Pardiso.jl](https://github.com/JuliaSparse/Pardiso.jl) or [SuiteSparseGraphBLAS.jl](https://docs.juliahub.com/SuiteSparseGraphBLAS/HtDaW/0.6.0/#Introduction). \n",
    "- <b> third approach </b>: use home-brewed [preconditioned conjugate gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method). Sequential version is available from [IterativeSolvers.jl]([https://github.com/JuliaLinearAlgebra/IterativeSolvers.jl). Use parallel BLAS1 and BLAS2 functions; using [sparse-matrix multiplication](https://github.com/JuliaInv/ParSpMatVec.jl);\n",
    "\n",
    "### Parallel BLAS-1 Operations \n",
    "Using [ParallelUtilities](https://docs.juliahub.com/ParallelUtilities/SO4iL/0.7.0/). \n",
    "\n",
    "#### Vector Norm \n",
    "Given vector ${\\mathbf v}$, compute its squared norm $\\| \\mathbf v \\|^2$, using pmapsum(x->x^2,v). Alternatively, this is the function dot with the same input argument repeated.  \n",
    "\n",
    "#### Vectors Inner Product \n",
    "Given vectors ${\\mathbf v}$ and ${\\mathbf w}$, compute their inner product ${\\mathbf v} \\cdot {\\mathbf w}$, using pmapsum((x,y)->x*y,v,w). See [this post](https://discourse.julialang.org/t/function-pmap-multi-argument/74153) for an example of pmap using two arguments. Alternatively, this is the function dot. \n",
    " \n",
    "#### Vector Update \n",
    "Given vectors ${\\mathbf v}$ and ${\\mathbf w}$, and given a number $\\alpha$ (typically the result of an inner product evaluation), compute the vector update ${\\mathbf w} = {\\mathbf w} + \\alpha {\\mathbf v}$, using pmapsum((x,y,a)->y+a*x,v,w,alpha). Alternatively, we can update the components of the vector ${\\mathbf w}$ elementwise. \n",
    "\n",
    "### Parallel BLAS-2 Operations \n",
    "\n",
    "#### Sparse Matrix-Vector Multiplication \n",
    "Use [ParSpMatVec](https://github.com/JuliaInv/ParSpMatVec.jl/blob/master/test/test_A_mul_B.jl) for sparse matrix vector multiplication. Alternatively, this is the function mul!. \n",
    "Background information is provided in e.g. [Parallel Linear Algebra by Deprez](https://fdesprez.github.io/teaching/par-comput/lectures/slides/L8-AlLinPar-2p.pdf); \n",
    "\n",
    "### Steepest Descent Method \n",
    "Combine ingredients above to implement the steepest descent method as described in Section [Solution to a Linear System](https://en.wikipedia.org/wiki/Gradient_descent#Solution_of_a_linear_system). \n",
    "\n",
    "### Conjugate Gradient Method \n",
    "Extend steepest descent to the conjugate gradient method as described in Section [Resulting Algorithm](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm). An explanation of the CG algorithm using iterates in explained on [iterative-methods-done-right](https://lostella.github.io/2018/07/25/iterative-methods-done-right.html). Note use PCG with proper [overlap of computation and communication](https://netlib.org/linalg/html_templates/node107.html#SECTION00941100000000000000);  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Distributed Memory Linear System Solve \n",
    "Using the PETSc library (as done by the bachelor students). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Postprocessing \n",
    "Visualize the computed solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
